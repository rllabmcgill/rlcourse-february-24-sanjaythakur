{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Discount-factor\n",
    "GAMMA = 0.95\n",
    "\n",
    "#ALPHA or step-size\n",
    "ALPHA = 0.1\n",
    "\n",
    "#Defining the EPSILON which would ensure regular exploration. Our EPSILON will decrease linearly with each iteration of a episode and will eventually fade away to 0 .\n",
    "EPSILON = 1\n",
    "\n",
    "#Number of episodes to consider\n",
    "TOTAL_EPISODES_TO_CONSIDER = 1000\n",
    "\n",
    "#Maximum allowed episode length\n",
    "MAXIMUM_EPISODE_LENGTH = 100\n",
    "\n",
    "#Number of planning steps\n",
    "NUMBER_PLANNING_STEPS = 50\n",
    "\n",
    "#All possible actions defined\n",
    "ACTION_UP = 'UP'\n",
    "ACTION_DOWN = 'DOWN'\n",
    "ACTION_LEFT = 'LEFT'\n",
    "ACTION_RIGHT = 'RIGHT'\n",
    "\n",
    "#Start and end of any episode\n",
    "START_STATE = '00'\n",
    "END_STATE = '15'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_states = ['00', '01', '02', '03',\n",
    "          '04', '05', '06', '07',\n",
    "          '08', '09', '10', '11',\n",
    "          '12', '13', '14', '15']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to mimic the environment. Remember that these parameters won't be known to us, not at least before we start moving our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "immediate_state_rewards =  {\n",
    "    '00': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '01': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '02': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '03': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '04': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '05': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '06': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '07': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '08': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '09': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '10': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '11': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '12': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '13': {ACTION_UP : 0, ACTION_RIGHT : 0, ACTION_DOWN: 1, ACTION_LEFT: 0},\n",
    "    '14': {ACTION_UP : 0, ACTION_RIGHT : 1, ACTION_DOWN: 0, ACTION_LEFT: 0},\n",
    "    '15': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "}\n",
    "\n",
    "all_transitions =  {\n",
    "    '00': {ACTION_UP : '00', ACTION_RIGHT : '01', ACTION_DOWN: '04', ACTION_LEFT: '00'},\n",
    "    '01': {ACTION_UP : '01', ACTION_RIGHT : '02', ACTION_DOWN: '05', ACTION_LEFT: '00'},\n",
    "    '02': {ACTION_UP : '02', ACTION_RIGHT : '03', ACTION_DOWN: '06', ACTION_LEFT: '01'},\n",
    "    '03': {ACTION_UP : '03', ACTION_RIGHT : '03', ACTION_DOWN: '07', ACTION_LEFT: '02'},\n",
    "    '04': {ACTION_UP : '00', ACTION_RIGHT : '05', ACTION_DOWN: '08', ACTION_LEFT: '04'},\n",
    "    '05': {ACTION_UP : '01', ACTION_RIGHT : '06', ACTION_DOWN: '09', ACTION_LEFT: '04'},\n",
    "    '06': {ACTION_UP : '02', ACTION_RIGHT : '07', ACTION_DOWN: '10', ACTION_LEFT: '05'},\n",
    "    '07': {ACTION_UP : '03', ACTION_RIGHT : '07', ACTION_DOWN: '11', ACTION_LEFT: '06'},\n",
    "    '08': {ACTION_UP : '04', ACTION_RIGHT : '09', ACTION_DOWN: '12', ACTION_LEFT: '08'},\n",
    "    '09': {ACTION_UP : '05', ACTION_RIGHT : '10', ACTION_DOWN: '13', ACTION_LEFT: '08'},\n",
    "    '10': {ACTION_UP : '06', ACTION_RIGHT : '11', ACTION_DOWN: '14', ACTION_LEFT: '09'},\n",
    "    '11': {ACTION_UP : '07', ACTION_RIGHT : '11', ACTION_DOWN: '15', ACTION_LEFT: '10'},\n",
    "    '12': {ACTION_UP : '08', ACTION_RIGHT : '13', ACTION_DOWN: '12', ACTION_LEFT: '12'},\n",
    "    '13': {ACTION_UP : '09', ACTION_RIGHT : '14', ACTION_DOWN: '13', ACTION_LEFT: '12'},\n",
    "    '14': {ACTION_UP : '10', ACTION_RIGHT : '15', ACTION_DOWN: '14', ACTION_LEFT: '13'},\n",
    "    '15': {ACTION_UP : '15', ACTION_RIGHT : '15', ACTION_DOWN: '15', ACTION_LEFT: '15'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters that are affected by the direct reinforcement learning step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_action_value_pairs = {\n",
    "    '00': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '01': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '02': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '03': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '04': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '05': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '06': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '07': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '08': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '09': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '10': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '11': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '12': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '13': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '14': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "    '15': {ACTION_UP : 1, ACTION_RIGHT : 1, ACTION_DOWN: 1, ACTION_LEFT: 1},\n",
    "}\n",
    "\n",
    "# We initialize our policy. We initialize our state action values with all optimistic values so that we don't get stuck at any deadlocks.\n",
    "greedy_policy = {\n",
    "    '00': ACTION_UP,\n",
    "    '01': ACTION_UP,\n",
    "    '02': ACTION_UP,\n",
    "    '03': ACTION_UP,\n",
    "    '04': ACTION_UP,\n",
    "    '05': ACTION_UP,\n",
    "    '06': ACTION_UP,\n",
    "    '07': ACTION_UP,\n",
    "    '08': ACTION_UP,\n",
    "    '09': ACTION_UP,\n",
    "    '10': ACTION_UP,\n",
    "    '11': ACTION_UP,\n",
    "    '12': ACTION_UP,\n",
    "    '13': ACTION_UP,\n",
    "    '14': ACTION_UP,\n",
    "    '15': ACTION_UP,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All model related variables in Dyna architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_state_action_value_pairs = {\n",
    "    '00': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '01': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '02': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '03': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '04': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '05': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '06': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '07': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '08': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '09': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '10': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '11': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '12': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '13': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '14': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "    '15': {ACTION_UP : (), ACTION_RIGHT : (), ACTION_DOWN: (), ACTION_LEFT: ()},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def updatePolicy():\n",
    "    for state, action_values in state_action_value_pairs.items():\n",
    "        highest_valued_action = ACTION_UP\n",
    "        highest_value = action_values[ACTION_UP]\n",
    "        if highest_value < action_values[ACTION_RIGHT]:\n",
    "            highest_valued_action = ACTION_RIGHT\n",
    "            highest_value = action_values[ACTION_RIGHT]\n",
    "        if highest_value < action_values[ACTION_DOWN]:\n",
    "            highest_valued_action = ACTION_DOWN\n",
    "            highest_value = action_values[ACTION_DOWN]\n",
    "        if highest_value < action_values[ACTION_LEFT]:\n",
    "            highest_valued_action = ACTION_LEFT\n",
    "            highest_value = action_values[ACTION_LEFT]\n",
    "            \n",
    "        greedy_policy[state] = highest_valued_action\n",
    "\n",
    "def chooseActionStochastically():\n",
    "    random_throw = random.uniform(0, 1)\n",
    "    if random_throw < 0.25:\n",
    "        return ACTION_UP\n",
    "    elif random_throw < 0.5:\n",
    "        return ACTION_RIGHT\n",
    "    elif random_throw < 0.75:\n",
    "        return ACTION_DOWN\n",
    "    else:\n",
    "        return ACTION_LEFT\n",
    "    \n",
    "def takeAction(state, action):\n",
    "    reward = immediate_state_rewards[state][action]\n",
    "    next_state = all_transitions[state][action]\n",
    "    return next_state, reward\n",
    "\n",
    "def printPolicy():\n",
    "    print(\"Updated Policy\", end = '')\n",
    "    for state in all_states:\n",
    "        if (int(state) % 4) == 0:\n",
    "            print(\"\\n\")\n",
    "        print(state, \"::\", greedy_policy[state],\"\\t\", end = '')\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validated\n",
      "Validated\n",
      "Validated\n",
      "Validated\n",
      "Updated Policy\n",
      "\n",
      "00 :: DOWN \t01 :: DOWN \t02 :: DOWN \t03 :: DOWN \t\n",
      "\n",
      "04 :: RIGHT \t05 :: DOWN \t06 :: DOWN \t07 :: LEFT \t\n",
      "\n",
      "08 :: DOWN \t09 :: DOWN \t10 :: DOWN \t11 :: LEFT \t\n",
      "\n",
      "12 :: RIGHT \t13 :: DOWN \t14 :: LEFT \t15 :: UP \t\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPSILON = 1\n",
    "\n",
    "#Number of episodes to consider\n",
    "TOTAL_EPISODES_TO_CONSIDER = 5\n",
    "\n",
    "#Maximum allowed episode length\n",
    "MAXIMUM_EPISODE_LENGTH = 100\n",
    "\n",
    "#Number of planning steps\n",
    "NUMBER_PLANNING_STEPS = 50\n",
    "\n",
    "\n",
    "\n",
    "all_observed_state_action_pairs = set()\n",
    "\n",
    "for episode_iterator in range(TOTAL_EPISODES_TO_CONSIDER):\n",
    "    \n",
    "    EPSILON = (1/((0.2 * episode_iterator) + 1))\n",
    "    \n",
    "    current_state = START_STATE\n",
    "    current_episode_length = 0\n",
    "    while(current_state != END_STATE and current_episode_length < MAXIMUM_EPISODE_LENGTH):\n",
    "        current_episode_length += 1\n",
    "        \n",
    "        random_throw = random.uniform(0, 1)\n",
    "        if random_throw < EPSILON:\n",
    "            current_action = chooseActionStochastically()\n",
    "        else:\n",
    "            current_action = greedy_policy[current_state]\n",
    "    \n",
    "        next_state, reward = takeAction(current_state, current_action)\n",
    "        \n",
    "        state_action_value_pairs[current_state][current_action] = state_action_value_pairs[current_state][current_action] + (ALPHA * (reward + (GAMMA * state_action_value_pairs[next_state][greedy_policy[next_state]]) - state_action_value_pairs[current_state][current_action]))\n",
    "        \n",
    "        all_observed_state_action_pairs.add((current_state, current_action))\n",
    "        \n",
    "        model_state_action_value_pairs[current_state][current_action] = (reward, next_state)\n",
    "        \n",
    "        for planning_iterator in range(NUMBER_PLANNING_STEPS):\n",
    "            random_picker = int(random.uniform(0, len(all_observed_state_action_pairs)))\n",
    "            state_action_pair = list(all_observed_state_action_pairs)[random_picker]\n",
    "            _state = state_action_pair[0]\n",
    "            _action = state_action_pair[1]\n",
    "            _reward, _next_state = model_state_action_value_pairs[_state][_action]\n",
    "            \n",
    "            state_action_value_pairs[_state][_action] = state_action_value_pairs[_state][_action] + (ALPHA * (_reward + (GAMMA * state_action_value_pairs[_next_state][greedy_policy[_next_state]]) - state_action_value_pairs[_state][_action]))\n",
    "        \n",
    "        if next_state == END_STATE:\n",
    "            print(\"Validated\")\n",
    "            \n",
    "        current_state = next_state\n",
    "        updatePolicy()\n",
    "        \n",
    "printPolicy()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
